{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Query → Solr Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will implement two pipelines for the NLQ -> SQ translation task. The first is a custom LangChain chain that explicitly combines all the components that are used to accomplish the task. The second is a more convenient, streamlined approach that takes advantage of the LLMChain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_ads\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain (No Chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Parser\n",
    "\n",
    "This is not the ideal implementation. The output parser would ideally be a separate module in the chain. Here, we just inject the format instructions into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "# generate format instructions for the prompt\n",
    "query_schema = ResponseSchema(\n",
    "    name=\"q\",\n",
    "    description=\"The structured query based on Human input to be sent to ADS\",\n",
    ")\n",
    "response_schemas = [query_schema]\n",
    "output_parser = StructuredOutputParser(response_schemas=response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"q\": string  // The structured query based on Human input to be sent to ADS\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameterize Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTIONS: \n",
      "The following is a conversation between a human and an AI. The AI should answer the question based on the context, examples, and current conversation provided. If the AI does not know the answer to a question, it truthfully says it does not know. \n",
      "\n",
      "CONTEXT: \n",
      "The AI is an expert database search engineer. Specifically, the AI is trained to create structured queries that are submitted to NASA Astrophysics Data System (ADS), a digital library portal for researchers in astronomy and physics. The ADS system accepts queries using the Apache Solr search syntax. \n",
      "Here are all available fields and operators in the ADS database, where each field is separated by a space in this list: {fields} {operators}\n",
      "{multi_query_paragraph}\n",
      "\n",
      "AVAILABLE FIELDS: \n",
      "Here is an example for each of the available fields in the ADS database. The formatting is a Python list of lists. The inner list corresponds to an available field, is five elements long, and each element starts and ends with a single quote e.g. '. The first element is keywords associated with the field, the second element is the query syntax, the third element is the example query, the fourth element is associated notes, and the fifth element is the field name:  {fields_examples}\n",
      "AVAILABLE OPERATORS: Here is an example for each of the available operators in the ADS database. The formatting is a Python list of lists. The inner list corresponds to an available operator, is three elements long, and each element starts and ends with a single quote e.g. '. The first element is the operator name, the second element is the example query, and the third element is associated notes:  {operators_examples}\n",
      "\n",
      "EXAMPLES:\n",
      "The examples below are references for a typical, singular Human and AI interaction that provides the correct answer to a Human question.\n",
      "{specific_examples}\n",
      "The AI should create a similar query based on the question from the user.\n",
      "{explanation}\n",
      "{format_instructions}\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load promp template\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_path = '../../data/forward_templates/forward_prompt_simple_history.yaml'\n",
    "\n",
    "# Load the prompt template from a file. It is a PromptTemplate\n",
    "prompt_template = load_prompt(template_path)\n",
    "print(prompt_template.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_input_vars = {\n",
    "    \"fields\": download_ads.get_fields_names(),\n",
    "    \"operators\": download_ads.get_operator_names(),\n",
    "    \"multi_query_paragraph\": download_ads.get_multi_query_paragraph(), \n",
    "    \"fields_examples\": str(download_ads.get_examples()),\n",
    "    \"operators_examples\": str(download_ads.get_operators_info()[\"name_example_explanation\"]),\n",
    "    \"explanation\": \"The AI should only output the answer and no additional information.\",\n",
    "    \"format_instructions\": format_instructions   \n",
    "}\n",
    "\n",
    "partial_template = prompt_template.partial(**partial_input_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def create_model(temperature: float = 0.0, model_name: str = \"gpt-3.5-turbo\") -> ChatOpenAI:\n",
    "    return ChatOpenAI(temperature=temperature, model=model_name)\n",
    "\n",
    "llm = create_model(temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_choices = {\n",
    "    \"HF\": {\"model\": HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"), \"dim\": 384},\n",
    "    \"OpenAI\": {\n",
    "        \"model\": OpenAIEmbeddings(),  # using this is annoying since rate limiting of 3/minute\n",
    "        \"dim\": 1536,\n",
    "    },\n",
    "}\n",
    "\n",
    "embedding = embedding_choices[\"HF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "\n",
    "def get_pinecone_langchain_client(index_name: str, embedding, embedding_dim) -> Pinecone:\n",
    "    # Initialize pinecone module\n",
    "    pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENV\"]\n",
    "    )\n",
    "\n",
    "    # Get or create pinecone index\n",
    "    if index_name not in pinecone.list_indexes():\n",
    "        pinecone.create_index(\n",
    "            name=index_name,\n",
    "            metric=\"cosine\",\n",
    "            dimension=embedding_dim,\n",
    "        )\n",
    "    index = pinecone.Index(index_name)\n",
    "\n",
    "    # Create LangChain pinecone client\n",
    "    # note: embedding.embed_query is the function that's called to do the embedding\n",
    "    pinecone_vectorstore = Pinecone(index=index, embedding=embedding, text_key=\"text\")\n",
    "    return pinecone_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_vectorstore = get_pinecone_langchain_client(\"demo\", embedding=embedding[\"model\"], embedding_dim=embedding[\"dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_from_file(file_path: str) -> list[Document]:\n",
    "    examples = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as stream:\n",
    "            examples = yaml.safe_load(stream)\n",
    "\n",
    "        print(examples)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing YAML file: {exc}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: file not found: {file_path}\")\n",
    "\n",
    "    documents = []\n",
    "    for example in examples:\n",
    "        documents.append(\n",
    "            Document(page_content=example['text'], metadata={'solr': example['solr']})\n",
    "        )\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'finds articles published between 1980 and 1990 by John Huchra', 'solr': '```json\\n{\\n\"q\": \"author:\\\\\"Huchra, John\\\\\" year:1980-1990\"\\n}\\n```'}, {'text': 'What are papers that mention neural networks in the abstract?', 'solr': '```json\\n{\\n\"q\": \"abs:\\\\\"neural networks\\\\\"\"\\n}\\n```'}, {'text': 'Give me papers that mention neural networks in the title or keywords or abstract', 'solr': '```json\\n{\\n\"q\": \"abs:\\\\\"neural networks\\\\\"\"\\n}\\n```'}, {'text': 'Papers with that contain neural networks in the full text', 'solr': '```json\\n{\\n\"q\": \"body:\\\\\"neural networks\\\\\"\"\\n}\\n```'}, {'text': 'Everything from 2002 to 2008', 'solr': '```json\\n{\\n\"q\": \"year:2002-2008\"\\n}\\n```'}, {'text': 'What papers by Kurtz, et al discuss weak lensing?', 'solr': '```json\\n{\\n\"q\": \"author:\\\\\"Kurtz\\\\\" abs:\\\\\"weak lensing\\\\\"\"\\n}\\n```'}, {'text': 'What papers by Alberto, et al discuss astronomy?', 'solr': '```json\\n{\\n\"q\": \"author:\\\\\"Alberto\\\\\" abs:\\\\\"astronomy\\\\\"\"\\n}\\n```'}, {'text': 'Show me papers about exoplanets with data from the MAST archive', 'solr': '```json\\n{\\n\"q\": \"abs:\\\\\"exoplanets\\\\\" data:MAST\"\\n}\\n```'}, {'text': 'Give me papers which are like those from 2003AJ....125..525J', 'solr': '```json\\n{\\n\"q\": \"similar(bibcode:2003AJ....125..525J)\"\\n}\\n```'}, {'text': 'Find me papers which can help me understand neural networks', 'solr': '```json\\n{\\n\"q\": \"useful:\\\\\"neural networks\\\\\"\"\\n}\\n```'}, {'text': 'return the top 100 most cited astronomy papers', 'solr': '```json\\n{\\n\"q\": \"topn(100, database:astronomy, citation_count desc)\"\\n}\\n```'}, {'text': 'Give me the most important paper from Pavlos in 2020 100 most cited astronomy papers', 'solr': '```json\\n{\\n\"q\": \"topn(1, author:\\\\\"Pavlos\\\\\", citation_count desc)\"\\n}\\n```'}, {'text': 'Give me the top 3 papers from Alberto between 2018 and 2022', 'solr': '```json\\n{\\n\"q\": \"author:\\\\\"Alberto\\\\\" year:2018-2022\",\\n\"rows\": 3,\\n\"sort\": \"citation_count desc\"\\n}\\n```'}]\n"
     ]
    }
   ],
   "source": [
    "documents = documents_from_file('../../data/examples.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c072db95-a7d6-4320-a28f-d20d3cbee589',\n",
       " '28ecbd8a-e057-4528-977a-4857becfef13',\n",
       " '48804b9e-0482-433d-946a-ce1a628b47eb',\n",
       " 'd35db382-ba40-401e-8163-375816ba5dc2',\n",
       " '3aacbd38-14cd-4328-bfb2-7967ec1ab333',\n",
       " '20c00b40-1b82-4fe7-97e2-f2059ce72d2c',\n",
       " '3ae10e25-23e3-477b-b8b0-fb1a855fa269',\n",
       " '212f9d8d-a2b6-4796-96ff-1ca9f901c96e',\n",
       " '730bb67a-c169-4292-8ffd-fb5b8b5d444a',\n",
       " 'b0a50722-d5fb-4925-b811-946f2c6c5765',\n",
       " '79fe5132-44bc-4ff6-b2e2-08c775a49bb3',\n",
       " 'eaa67af6-9c6e-4f35-985f-c907eb02c1c3',\n",
       " 'fa2158f8-0285-478b-ad20-2ce7dfdb94f7']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add documents to the vector database\n",
    "pinecone_vectorstore.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output_parser \u001b[39m=\u001b[39m output_parser\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m chain \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mspecific_examples\u001b[39m\u001b[39m\"\u001b[39m: itemgetter(\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m|\u001b[39m retriever,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: itemgetter(\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m } \u001b[39m|\u001b[39m prompt \u001b[39m|\u001b[39m model \u001b[39m|\u001b[39m output_parser\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m result \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: question})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/adschat-jmOWAmzn/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1153\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1153\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1154\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1155\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1156\u001b[0m             patch_config(\n\u001b[1;32m   1157\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1158\u001b[0m             ),\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/adschat-jmOWAmzn/lib/python3.11/site-packages/langchain/schema/prompt_template.py:58\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m     59\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_prompt(\n\u001b[1;32m     60\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[1;32m     61\u001b[0m         ),\n\u001b[1;32m     62\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m     63\u001b[0m         config,\n\u001b[1;32m     64\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     65\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/adschat-jmOWAmzn/lib/python3.11/site-packages/langchain/schema/runnable/base.py:668\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    662\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    663\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    664\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[1;32m    665\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    666\u001b[0m )\n\u001b[1;32m    667\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m    669\u001b[0m         func, \u001b[39minput\u001b[39;49m, config, run_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    671\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    672\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/adschat-jmOWAmzn/lib/python3.11/site-packages/langchain/schema/runnable/config.py:259\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    258\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/adschat-jmOWAmzn/lib/python3.11/site-packages/langchain/schema/prompt_template.py:60\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m     59\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[0;32m---> 60\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_variables}\n\u001b[1;32m     61\u001b[0m         ),\n\u001b[1;32m     62\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m     63\u001b[0m         config,\n\u001b[1;32m     64\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/adschat-jmOWAmzn/lib/python3.11/site-packages/langchain/schema/prompt_template.py:60\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Dict, config: RunnableConfig \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PromptValue:\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m     59\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_prompt(\n\u001b[0;32m---> 60\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{key: inner_input[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_variables}\n\u001b[1;32m     61\u001b[0m         ),\n\u001b[1;32m     62\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m     63\u001b[0m         config,\n\u001b[1;32m     64\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'history'"
     ]
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "retriever = pinecone_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "prompt = partial_template\n",
    "model = llm\n",
    "question = \"Papers that contain neural networks in the full text\"\n",
    "output_parser = output_parser\n",
    "\n",
    "chain = {\n",
    "    \"specific_examples\": itemgetter(\"question\") | retriever,\n",
    "    \"input\": itemgetter(\"question\")\n",
    "} | prompt | model | output_parser\n",
    "\n",
    "result = chain.invoke({\"question\": question})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'q\\': \\'body:\"neural networks\"\\'}'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_str = str(result)\n",
    "result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'q\\': \\'body:\"neural networks\"\\'}'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_repr = repr(result)\n",
    "result_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Result to File to See JSON Encoding\n",
    "\n",
    "The string written to file is something that should be able to be encoded as a json object, and when encoded should consist of a string representing a json object. If we wanted to convert this string back into python object, we could just do `dict(json_str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         json\u001b[39m.\u001b[39mdump(result_str, f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m# s = json.dumps(result)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# json.dump(s, f)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m# f.write(str(result))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m result_to_ls_format(out_file, result)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# with open(out_file, 'w') as f:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     s = json.dumps(result)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#     print(s)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#     json.dump(o, f)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#     # f.write(s)\u001b[39;00m\n",
      "\u001b[1;32m/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb Cell 27\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult_to_ls_format\u001b[39m(file_path, result):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     result_str \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(result)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     r2 \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39;49m(result_str)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(r2 \u001b[39m==\u001b[39m result)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tannermarsh/thesis/adschat/src/ads_chatbot/forward.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "import json\n",
    "out_file = \"../../data/result.json\"\n",
    "\n",
    "def result_to_ls_format(file_path, result):\n",
    "\n",
    "    result_str = str(result)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(result_str, f)\n",
    "        # s = json.dumps(result)\n",
    "        # json.dump(s, f)\n",
    "        # f.write(str(result))\n",
    "        \n",
    "\n",
    "result_to_ls_format(out_file, result)\n",
    "# with open(out_file, 'w') as f:\n",
    "#     s = json.dumps(result)\n",
    "#     print(s)\n",
    "#     f.write(s)\n",
    "#     o = {\"solr_query\": s}\n",
    "#     json.dump(s, f)\n",
    "#     f.write('\\n')\n",
    "#     json.dump(o, f)\n",
    "#     # f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the promp template\n",
    "# print(str(prompt.template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Bob! How can I assist you today?')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"output\": response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi im bob'),\n",
       "  AIMessage(content='Hi Bob! How can I assist you today?')]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"whats my name\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough() |\n",
    "    {\n",
    "    \"history\" : RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input\": \"hi im bob\", \"specific_examples\": \"none\"}\n",
    "response = chain.invoke(inputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Chain with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(x):\n",
    "    print(x)\n",
    "    return x\n",
    "    # print(inputs)\n",
    "    # print(response)\n",
    "    # memory.save_context(inputs, {\"output\": response.content})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_history_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        specific_examples= itemgetter(\"specific_examples\") | retriever\n",
    "    )\n",
    "    | partial_template\n",
    "    # | model\n",
    "    | RunnableLambda(update_memory)\n",
    "    # | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='INSTRUCTIONS: \\nThe following is a conversation between a human and an AI. The AI should answer the question based on the context, examples, and current conversation provided. If the AI does not know the answer to a question, it truthfully says it does not know. \\n\\nCONTEXT: \\nThe AI is an expert database search engineer. Specifically, the AI is trained to create structured queries that are submitted to NASA Astrophysics Data System (ADS), a digital library portal for researchers in astronomy and physics. The ADS system accepts queries using the Apache Solr search syntax. \\nHere are all available fields and operators in the ADS database, where each field is separated by a space in this list: abs abstract ack aff aff_id alternate_bibcode alternate_title arXiv arxiv_class author author author_count author_count bibcode bibgroup bibstem body citation_count citation_count copyright data database pubdate doctype doi ^ full grant identifier inst issue keyword lang object orcid orcid_pub orcid_user orcid_other page bibstem property read_count title vizier volume year year citations pos references reviews similar topn trending useful docs\\nYou can string together any number of search terms to develop a query.  By default search terms will be combined using AND as the default boolean operator, but this can be changed by explicitly specifying OR beween them.  Similarly one can exclude a term by prepending a “-“ sign to it (or using the boolean “NOT”).  Multiple search words or phrases may be grouped in a fielded query by enclosing them in parenthesis. \\n\\nAVAILABLE FIELDS: \\nHere is an example for each of the available fields in the ADS database. The formatting is a Python list of lists. The inner list corresponds to an available field, is five elements long, and each element starts and ends with a single quote e.g. \\'. The first element is keywords associated with the field, the second element is the query syntax, the third element is the example query, the fourth element is associated notes, and the fifth element is the field name:  [[\\'Abstract/Title/Keywords\\', \\'abs:“phrase”\\', \\'abs:“dark energy”\\', \\'search for word or phrase in abstract, title and keywords\\'], [\\'Abstract\\', \\'abstract:“phrase”\\', \\'abstract:“dark energy”\\', \\'search for a word or phrase in an abstract only\\'], [\\'Acknowledgements\\', \\'ack:“phrase”\\', \\'ack:“ADS”\\', \\'search for a word or phrase in the acknowledgements\\'], [\\'Affiliation\\', \\'aff:“phrase”\\', \\'aff:“harvard”\\', \\'search for word or phrase in the raw, provided affiliation field\\'], [\\'Affiliation ID\\', \\'aff_id:ID\\', \\'aff_id:A00211\\', \\'search for an affiliation ID listed in the Canonical Affiliations list in the child column. This field will soon also accept 9-digit ROR ids.\\'], [\\'Alternate Bibcode\\', \\'alternate_bibcode:adsbib\\', \\'alternate_bibcode:2003AJ….125..525J\\', \\'finds articles that used to (or still have) this bibcode\\'], [\\'Alternate Title\\', \\'alternate_title:“phrase”\\', \\'alternate_title:“Gammablitz”\\', \\'search for a word or phrase in an articles title if they have more than one, in multiple languages\\'], [\\'arXiv ID\\', \\'arXiv:arxivid\\', \\'arXiv:1108.0669\\', \\'finds a specific record using its arXiv id\\'], [\\'arXiv Class\\', \\'arxiv_class:arxivclass\\', \\'arxiv_class:“High Energy Physics - Experiment”\\', \\'finds all arXiv pre-prints in the class specified\\'], [\\'Author\\', \\'author:“Last, F”\\', \\'author:“huchra, j”\\', \\'author name may include just lastname and initial\\'], [\\'Author (cont.)\\', \\'author:“Last, First […]”\\', \\'author:“huchra, john p”\\', \\'an example of stricter author search (recommended)\\'], [\\'Author count\\', \\'author_count:count\\', \\'author_count:40\\', \\'find records that have a specific number of authors\\'], [\\'Author count (cont.)\\', \\'author_count:[min_count TO max_count]\\', \\'author_count:[10 TO 100]\\', \\'find records that have a range of author counts\\'], [\\'Bibcode\\', \\'bibcode:adsbib\\', \\'bibcode:2003AJ….125..525J\\', \\'finds a specific record using the ADS bibcode\\'], [\\'Bibliographic groups\\', \\'bibgroup:name\\', \\'bibgroup:HST\\', \\'limit search to papers in HST bibliography (*)\\'], [\\'Bibliographic stem\\', \\'bibstem:adsbibstem\\', \\'bibstem:ApJ\\', \\'find records that contain a specific bibstem in their bibcode\\'], [\\'Body\\', \\'body:“phrase”\\', \\'body:“gravitational waves”\\', \\'search for a word or phrase in (only) the full text\\'], [\\'Citation count\\', \\'citation_count:count\\', \\'citation_count:40\\', \\'find records that have a specific number of citations\\'], [\\'Citation count (cont.)\\', \\'citation_count:[min_count TO max_count]\\', \\'citation_count:[10 TO 100]\\', \\'find records that have a range of citation counts\\'], [\\'Copyright\\', \\'copyright:copyright\\', \\'copyright:2012\\', \\'search for articles with certain copyrights\\'], [\\'Data links\\', \\'data:archive\\', \\'data:NED\\', \\'limit search to papers with data from NED (*)\\'], [\\'Database\\', \\'database:DB\\', \\'database:astronomy\\', \\'limit search to either astronomy or physics or general\\'], [\\'Date Range\\', \\'pubdate:[YYYY-MM TO YYYY-MM]\\', \\'pubdate:[2005-10 TO 2006-09]\\', \\'use fine-grained dates for publication range\\'], [\\'Document type\\', \\'doctype:type\\', \\'doctype:catalog\\', \\'limit search to records corresponding to data catalogs (*)\\'], [\\'DOI\\', \\'doi:DOI\\', \\'doi:10.1086/345794\\', \\'finds a specific record using its digital object id\\'], [\\'First Author\\', \\'^Last, F         author:“^Last, F”\\', \\'^huchra, j   author:“^huchra, j”\\', \\'limit the search to first-author papers\\'], [\\'Fulltext\\', \\'full:“phrase”\\', \\'full:“gravitational waves”\\', \\'search for word or phrase in fulltext, acknowledgements, abstract, title and keywords\\'], [\\'Grant\\', \\'grant:grant\\', \\'grant:NASA\\', \\'finds papers with specific grants listed in them\\'], [\\'Identifiers\\', \\'identifier:bibcode\\', \\'identifier:2003AJ….125..525J\\', \\'finds a paper using any of its identifiers, arXiv, bibcode, doi, etc.\\'], [\\'Institution\\', \\'inst:“abbreviation”\\', \\'inst:“Harvard U”\\', \\'search the curated list of affiliations (e.g. STScI and “Space Telescope Science Institute” have been matched); the full list is in the Abbrev column in the Canonical Affiliations list\\'], [\\'Issue\\', \\'issue:number\\', \\'issue:10\\', \\'search for papers in a certain issue\\'], [\\'Keywords\\', \\'keyword:“phrase”\\', \\'keyword:sun\\', \\'search publisher- or author-supplied keywords\\'], [\\'Language\\', \\'lang:“language”\\', \\'lang:korean\\', \\'search for papers with a given language\\'], [\\'Object\\', \\'object:“object”\\', \\'object:Andromeda\\', \\'search for papers tagged with a specific astronomical object (as shown here) or at or near a set of coordinates (see Astronomical Objects and Position Search above)\\'], [\\'ORCiD iDs\\', \\'orcid:id\\', \\'orcid:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD\\'], [\\'ORCiD iDs from publishers\\', \\'orcid_pub:id\\', \\'orcid_pub:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD specified by a Publisher\\'], [\\'ORCiD iDs from known ADS users\\', \\'orcid_user:id\\', \\'orcid_id:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD claimed by known ADS users\\'], [\\'ORCiD iDs from uknknown ADS users\\', \\'orcid_other:id\\', \\'orcid_other:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD claimed by unknown ADS users\\'], [\\'Page\\', \\'page:number\\', \\'page:410\\', \\'search for papers with a given page number\\'], [\\'Publication\\', \\'bibstem:“abbrev”\\', \\'bibstem:ApJ\\', \\'limit search to a specific publication\\'], [\\'Properties\\', \\'property:type\\', \\'property:openaccess\\', \\'limit search to article with specific attributes (*)\\'], [\\'Read count\\', \\'read_count:count\\', \\'read_count:10\\', \\'search for papers with a given number of reads\\'], [\\'Title\\', \\'title:“phrase”\\', \\'title:“weak lensing”\\', \\'search for word or phrase in title field\\'], [\\'VizieR keywords\\', \\'vizier:“phrase”\\', \\'vizier:“Optical”\\', \\'search for papers with a given set of VizieR keywords\\'], [\\'Volume\\', \\'volume:volume\\', \\'volume:10\\', \\'search for papers with a given volume\\'], [\\'Year\\', \\'year:YYYY\\', \\'year:2000\\', \\'require specific publication year\\'], [\\'Year Range\\', \\'year:YYYY-YYYY\\', \\'year:2000-2005\\', \\'require publication date range\\']]\\nAVAILABLE OPERATORS: Here is an example for each of the available operators in the ADS database. The formatting is a Python list of lists. The inner list corresponds to an available operator, is three elements long, and each element starts and ends with a single quote e.g. \\'. The first element is the operator name, the second element is the example query, and the third element is associated notes:  [[\\'citations\\', \\'citations(aff:MIT)\\', \\'Returns list of citations for papers matching the inner query; use fl=[citations] to retrieve the field contents\\'], [\\'pos\\', \\'pos(author:accomazzi, 1, 5)\\', \\'The pos() operator allows you to search for an item within a field by specifying the position  (range). The syntax for this operator is pos(fieldedquery,position,[endposition]). If no endposition is given, then it is assumed to be endposition = position, otherwise this performs a query within the range [position, endposition].\\'], [\\'references\\', \\'references(author:huchra)\\', \\'Returns list of references from papers matching the inner query\\'], [\\'reviews\\', \\'reviews(title:\"monte carlo\")\\', \\'returns the list of documents citing the most relevant papers on the topic being researched; these are papers containing the most extensive reviews of the field.\\'], [\\'similar\\', \\'similar(title:hubble^2, abstract, 100)similar(\"hubble space telescope\", input)\\', \\'Find similar documents, either based on their similarity with the documents from the inner query or similar to the text that you supplied. Format: similar(queryOrText, fields, maxQueryTerms, docToSearch, minTermFreq, minDocFreq, percentToMatch). - queryOrText: string, this can be a query or input - fields: list of fields separated by spaces, or special token ‘input’ which means use the query as is, as input - maxQueryTerms: modifies similarity search, only this many terms will be considered during the search (those terms are NOT the first X collected, but they will be the first X terms weighted by TFIDF - term frequency/inverse document frequency) - docToSearch: how many documents to collect in the first phase, is ignored when fields=’input’ - minTermFreq: term is selected only if its frequency is this or higher - minDocFreq: selected term must be present in at least that many documents - percentToMatch: ratio of terms that have to be present in the selected documents, default is 0.0f. For example, if 100 terms was used to discover similar docs, and if the ratio was 0.3f - then 30 terms must be present in the docs that are returned.\\'], [\\'topn\\', \\'topn(200, citations(title:hubble), citation_count desc)\\', \"Limit results to the best top N (by their ranking or sort order); format: topn(int, query, \\'sort order\\'). If the sort order is not specified, the default score desc will be used.\"], [\\'trending\\', \\'trending(\"machine learning\")\\', \\'Trending – returns the list of documents most read by users who read recent papers on the topic being researched; these are papers currently being read by people interested in this field.\\'], [\\'useful\\', \\'useful(\"gradient descent\")\\', \\'Useful – returns the list of documents frequently cited by the most relevant papers on the topic being researched; these are studies which discuss methods and techniques useful to conduct research in this field.\\'], [\\'docs\\', \\'docs(library/hHGU1Ef-TpacAhicI3J8kQ)\\', \\'Retrieve set of documents specified by their IDs. You can think of this as a set operator; it will fill the set with documents that correspond to identifiers that are passed in. And this set can then be combined with other queries (i.e. docs(A) NOT author:huchra)\\']]\\n\\nEXAMPLES:\\nThe examples below are references for a typical, singular Human and AI interaction that provides the correct answer to a Human question.\\n[Document(page_content=\\'Everything from 2002 to 2008\\', metadata={\\'solr\\': \\'```json\\\\n{\\\\n\"q\": \"year:2002-2008\"\\\\n}\\\\n```\\'}), Document(page_content=\\'Everything from 2002 to 2008\\', metadata={\\'solr\\': \\'```json\\\\n{\\\\n\"q\": \"year:2002-2008\"\\\\n}\\\\n```\\'}), Document(page_content=\\'Everything from 2002 to 2008\\', metadata={\\'solr\\': \\'```json\\\\n{\\\\n\"q\": \"year:2002-2008\"\\\\n}\\\\n```\\'})]\\nThe AI should create a similar query based on the question from the user.\\nThe AI should only output the answer and no additional information.\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"q\": string  // The structured query based on Human input to be sent to ADS\\n}\\n```\\nCurrent conversation:\\n[]\\nHuman: hi im bob\\nAI:\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='INSTRUCTIONS: \\nThe following is a conversation between a human and an AI. The AI should answer the question based on the context, examples, and current conversation provided. If the AI does not know the answer to a question, it truthfully says it does not know. \\n\\nCONTEXT: \\nThe AI is an expert database search engineer. Specifically, the AI is trained to create structured queries that are submitted to NASA Astrophysics Data System (ADS), a digital library portal for researchers in astronomy and physics. The ADS system accepts queries using the Apache Solr search syntax. \\nHere are all available fields and operators in the ADS database, where each field is separated by a space in this list: abs abstract ack aff aff_id alternate_bibcode alternate_title arXiv arxiv_class author author author_count author_count bibcode bibgroup bibstem body citation_count citation_count copyright data database pubdate doctype doi ^ full grant identifier inst issue keyword lang object orcid orcid_pub orcid_user orcid_other page bibstem property read_count title vizier volume year year citations pos references reviews similar topn trending useful docs\\nYou can string together any number of search terms to develop a query.  By default search terms will be combined using AND as the default boolean operator, but this can be changed by explicitly specifying OR beween them.  Similarly one can exclude a term by prepending a “-“ sign to it (or using the boolean “NOT”).  Multiple search words or phrases may be grouped in a fielded query by enclosing them in parenthesis. \\n\\nAVAILABLE FIELDS: \\nHere is an example for each of the available fields in the ADS database. The formatting is a Python list of lists. The inner list corresponds to an available field, is five elements long, and each element starts and ends with a single quote e.g. \\'. The first element is keywords associated with the field, the second element is the query syntax, the third element is the example query, the fourth element is associated notes, and the fifth element is the field name:  [[\\'Abstract/Title/Keywords\\', \\'abs:“phrase”\\', \\'abs:“dark energy”\\', \\'search for word or phrase in abstract, title and keywords\\'], [\\'Abstract\\', \\'abstract:“phrase”\\', \\'abstract:“dark energy”\\', \\'search for a word or phrase in an abstract only\\'], [\\'Acknowledgements\\', \\'ack:“phrase”\\', \\'ack:“ADS”\\', \\'search for a word or phrase in the acknowledgements\\'], [\\'Affiliation\\', \\'aff:“phrase”\\', \\'aff:“harvard”\\', \\'search for word or phrase in the raw, provided affiliation field\\'], [\\'Affiliation ID\\', \\'aff_id:ID\\', \\'aff_id:A00211\\', \\'search for an affiliation ID listed in the Canonical Affiliations list in the child column. This field will soon also accept 9-digit ROR ids.\\'], [\\'Alternate Bibcode\\', \\'alternate_bibcode:adsbib\\', \\'alternate_bibcode:2003AJ….125..525J\\', \\'finds articles that used to (or still have) this bibcode\\'], [\\'Alternate Title\\', \\'alternate_title:“phrase”\\', \\'alternate_title:“Gammablitz”\\', \\'search for a word or phrase in an articles title if they have more than one, in multiple languages\\'], [\\'arXiv ID\\', \\'arXiv:arxivid\\', \\'arXiv:1108.0669\\', \\'finds a specific record using its arXiv id\\'], [\\'arXiv Class\\', \\'arxiv_class:arxivclass\\', \\'arxiv_class:“High Energy Physics - Experiment”\\', \\'finds all arXiv pre-prints in the class specified\\'], [\\'Author\\', \\'author:“Last, F”\\', \\'author:“huchra, j”\\', \\'author name may include just lastname and initial\\'], [\\'Author (cont.)\\', \\'author:“Last, First […]”\\', \\'author:“huchra, john p”\\', \\'an example of stricter author search (recommended)\\'], [\\'Author count\\', \\'author_count:count\\', \\'author_count:40\\', \\'find records that have a specific number of authors\\'], [\\'Author count (cont.)\\', \\'author_count:[min_count TO max_count]\\', \\'author_count:[10 TO 100]\\', \\'find records that have a range of author counts\\'], [\\'Bibcode\\', \\'bibcode:adsbib\\', \\'bibcode:2003AJ….125..525J\\', \\'finds a specific record using the ADS bibcode\\'], [\\'Bibliographic groups\\', \\'bibgroup:name\\', \\'bibgroup:HST\\', \\'limit search to papers in HST bibliography (*)\\'], [\\'Bibliographic stem\\', \\'bibstem:adsbibstem\\', \\'bibstem:ApJ\\', \\'find records that contain a specific bibstem in their bibcode\\'], [\\'Body\\', \\'body:“phrase”\\', \\'body:“gravitational waves”\\', \\'search for a word or phrase in (only) the full text\\'], [\\'Citation count\\', \\'citation_count:count\\', \\'citation_count:40\\', \\'find records that have a specific number of citations\\'], [\\'Citation count (cont.)\\', \\'citation_count:[min_count TO max_count]\\', \\'citation_count:[10 TO 100]\\', \\'find records that have a range of citation counts\\'], [\\'Copyright\\', \\'copyright:copyright\\', \\'copyright:2012\\', \\'search for articles with certain copyrights\\'], [\\'Data links\\', \\'data:archive\\', \\'data:NED\\', \\'limit search to papers with data from NED (*)\\'], [\\'Database\\', \\'database:DB\\', \\'database:astronomy\\', \\'limit search to either astronomy or physics or general\\'], [\\'Date Range\\', \\'pubdate:[YYYY-MM TO YYYY-MM]\\', \\'pubdate:[2005-10 TO 2006-09]\\', \\'use fine-grained dates for publication range\\'], [\\'Document type\\', \\'doctype:type\\', \\'doctype:catalog\\', \\'limit search to records corresponding to data catalogs (*)\\'], [\\'DOI\\', \\'doi:DOI\\', \\'doi:10.1086/345794\\', \\'finds a specific record using its digital object id\\'], [\\'First Author\\', \\'^Last, F         author:“^Last, F”\\', \\'^huchra, j   author:“^huchra, j”\\', \\'limit the search to first-author papers\\'], [\\'Fulltext\\', \\'full:“phrase”\\', \\'full:“gravitational waves”\\', \\'search for word or phrase in fulltext, acknowledgements, abstract, title and keywords\\'], [\\'Grant\\', \\'grant:grant\\', \\'grant:NASA\\', \\'finds papers with specific grants listed in them\\'], [\\'Identifiers\\', \\'identifier:bibcode\\', \\'identifier:2003AJ….125..525J\\', \\'finds a paper using any of its identifiers, arXiv, bibcode, doi, etc.\\'], [\\'Institution\\', \\'inst:“abbreviation”\\', \\'inst:“Harvard U”\\', \\'search the curated list of affiliations (e.g. STScI and “Space Telescope Science Institute” have been matched); the full list is in the Abbrev column in the Canonical Affiliations list\\'], [\\'Issue\\', \\'issue:number\\', \\'issue:10\\', \\'search for papers in a certain issue\\'], [\\'Keywords\\', \\'keyword:“phrase”\\', \\'keyword:sun\\', \\'search publisher- or author-supplied keywords\\'], [\\'Language\\', \\'lang:“language”\\', \\'lang:korean\\', \\'search for papers with a given language\\'], [\\'Object\\', \\'object:“object”\\', \\'object:Andromeda\\', \\'search for papers tagged with a specific astronomical object (as shown here) or at or near a set of coordinates (see Astronomical Objects and Position Search above)\\'], [\\'ORCiD iDs\\', \\'orcid:id\\', \\'orcid:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD\\'], [\\'ORCiD iDs from publishers\\', \\'orcid_pub:id\\', \\'orcid_pub:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD specified by a Publisher\\'], [\\'ORCiD iDs from known ADS users\\', \\'orcid_user:id\\', \\'orcid_id:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD claimed by known ADS users\\'], [\\'ORCiD iDs from uknknown ADS users\\', \\'orcid_other:id\\', \\'orcid_other:0000-0000-0000-0000\\', \\'search for papers that are associated with a specific ORCiD iD claimed by unknown ADS users\\'], [\\'Page\\', \\'page:number\\', \\'page:410\\', \\'search for papers with a given page number\\'], [\\'Publication\\', \\'bibstem:“abbrev”\\', \\'bibstem:ApJ\\', \\'limit search to a specific publication\\'], [\\'Properties\\', \\'property:type\\', \\'property:openaccess\\', \\'limit search to article with specific attributes (*)\\'], [\\'Read count\\', \\'read_count:count\\', \\'read_count:10\\', \\'search for papers with a given number of reads\\'], [\\'Title\\', \\'title:“phrase”\\', \\'title:“weak lensing”\\', \\'search for word or phrase in title field\\'], [\\'VizieR keywords\\', \\'vizier:“phrase”\\', \\'vizier:“Optical”\\', \\'search for papers with a given set of VizieR keywords\\'], [\\'Volume\\', \\'volume:volume\\', \\'volume:10\\', \\'search for papers with a given volume\\'], [\\'Year\\', \\'year:YYYY\\', \\'year:2000\\', \\'require specific publication year\\'], [\\'Year Range\\', \\'year:YYYY-YYYY\\', \\'year:2000-2005\\', \\'require publication date range\\']]\\nAVAILABLE OPERATORS: Here is an example for each of the available operators in the ADS database. The formatting is a Python list of lists. The inner list corresponds to an available operator, is three elements long, and each element starts and ends with a single quote e.g. \\'. The first element is the operator name, the second element is the example query, and the third element is associated notes:  [[\\'citations\\', \\'citations(aff:MIT)\\', \\'Returns list of citations for papers matching the inner query; use fl=[citations] to retrieve the field contents\\'], [\\'pos\\', \\'pos(author:accomazzi, 1, 5)\\', \\'The pos() operator allows you to search for an item within a field by specifying the position  (range). The syntax for this operator is pos(fieldedquery,position,[endposition]). If no endposition is given, then it is assumed to be endposition = position, otherwise this performs a query within the range [position, endposition].\\'], [\\'references\\', \\'references(author:huchra)\\', \\'Returns list of references from papers matching the inner query\\'], [\\'reviews\\', \\'reviews(title:\"monte carlo\")\\', \\'returns the list of documents citing the most relevant papers on the topic being researched; these are papers containing the most extensive reviews of the field.\\'], [\\'similar\\', \\'similar(title:hubble^2, abstract, 100)similar(\"hubble space telescope\", input)\\', \\'Find similar documents, either based on their similarity with the documents from the inner query or similar to the text that you supplied. Format: similar(queryOrText, fields, maxQueryTerms, docToSearch, minTermFreq, minDocFreq, percentToMatch). - queryOrText: string, this can be a query or input - fields: list of fields separated by spaces, or special token ‘input’ which means use the query as is, as input - maxQueryTerms: modifies similarity search, only this many terms will be considered during the search (those terms are NOT the first X collected, but they will be the first X terms weighted by TFIDF - term frequency/inverse document frequency) - docToSearch: how many documents to collect in the first phase, is ignored when fields=’input’ - minTermFreq: term is selected only if its frequency is this or higher - minDocFreq: selected term must be present in at least that many documents - percentToMatch: ratio of terms that have to be present in the selected documents, default is 0.0f. For example, if 100 terms was used to discover similar docs, and if the ratio was 0.3f - then 30 terms must be present in the docs that are returned.\\'], [\\'topn\\', \\'topn(200, citations(title:hubble), citation_count desc)\\', \"Limit results to the best top N (by their ranking or sort order); format: topn(int, query, \\'sort order\\'). If the sort order is not specified, the default score desc will be used.\"], [\\'trending\\', \\'trending(\"machine learning\")\\', \\'Trending – returns the list of documents most read by users who read recent papers on the topic being researched; these are papers currently being read by people interested in this field.\\'], [\\'useful\\', \\'useful(\"gradient descent\")\\', \\'Useful – returns the list of documents frequently cited by the most relevant papers on the topic being researched; these are studies which discuss methods and techniques useful to conduct research in this field.\\'], [\\'docs\\', \\'docs(library/hHGU1Ef-TpacAhicI3J8kQ)\\', \\'Retrieve set of documents specified by their IDs. You can think of this as a set operator; it will fill the set with documents that correspond to identifiers that are passed in. And this set can then be combined with other queries (i.e. docs(A) NOT author:huchra)\\']]\\n\\nEXAMPLES:\\nThe examples below are references for a typical, singular Human and AI interaction that provides the correct answer to a Human question.\\n[Document(page_content=\\'Everything from 2002 to 2008\\', metadata={\\'solr\\': \\'```json\\\\n{\\\\n\"q\": \"year:2002-2008\"\\\\n}\\\\n```\\'}), Document(page_content=\\'Everything from 2002 to 2008\\', metadata={\\'solr\\': \\'```json\\\\n{\\\\n\"q\": \"year:2002-2008\"\\\\n}\\\\n```\\'}), Document(page_content=\\'Everything from 2002 to 2008\\', metadata={\\'solr\\': \\'```json\\\\n{\\\\n\"q\": \"year:2002-2008\"\\\\n}\\\\n```\\'})]\\nThe AI should create a similar query based on the question from the user.\\nThe AI should only output the answer and no additional information.\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"q\": string  // The structured query based on Human input to be sent to ADS\\n}\\n```\\nCurrent conversation:\\n[]\\nHuman: hi im bob\\nAI:\\n')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_history_chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Manual Vector lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar(vectorstore, query: str, top_k: int = 3) -> list[Document]:\n",
    "    docs = vectorstore.similarity_search_with_score(query, k=top_k)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Papers with that contain neural networks in the full text', metadata={'solr': '```json\\n{\\n\"q\": \"body:\\\\\"neural networks\\\\\"\"\\n}\\n```'}),\n",
       "  0.983315647),\n",
       " (Document(page_content='Papers with that contain neural networks in the full text', metadata={'solr': '```json\\n{\\n\"q\": \"body:\\\\\"neural networks\\\\\"\"\\n}\\n```'}),\n",
       "  0.983315527),\n",
       " (Document(page_content='What are papers that mention neural networks in the abstract?', metadata={'solr': '```json\\n{\\n\"q\": \"abs:\\\\\"neural networks\\\\\"\"\\n}\\n```'}),\n",
       "  0.844233811)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_similar(pinecone_vectorstore, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Example Parsing for Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def yaml_to_label_studio(file_path: str):\n",
    "    examples = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as stream:\n",
    "            examples = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing YAML file: {exc}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: file not found: {file_path}\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = yaml_to_label_studio('../../data/examples.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'finds articles published between 1980 and 1990 by John Huchra',\n",
       " 'solr': '```json\\n{\\n\"q\": \"author:\\\\\"Huchra, John\\\\\" year:1980-1990\"\\n}\\n```'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"q\": \"author:\\\"Huchra, John\\\" year:1980-1990\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(examples[0]['solr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adschat-jmOWAmzn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
